{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_selected_PM_data(data_path):\n",
    "    PM_data = {}\n",
    "    data_name_lst = [name for name in os.listdir(data_path) if name.startswith('Fill')]\n",
    "    for name in data_name_lst:\n",
    "        a_df = pd.read_csv(os.path.join(data_path, name), sheet_name='PM2.5')\n",
    "        PM_data[name.split('_')[1]] = a_df\n",
    "    return PM_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_label_station(PM_data_year, label_df):\n",
    "    label_dict = {'Date':PM_data_year['Date']}\n",
    "    station_columns = PM_data_year.columns[1:]\n",
    "    for station_id in station_columns:\n",
    "        the_label = label_df['label'][label_df['station_id'] == station_id].to_list()[0]\n",
    "        if str(the_label) not in label_dict.keys():\n",
    "            label_dict[str(the_label)] = PM_data_year[station_id]\n",
    "        else:\n",
    "            label_dict[str(the_label)] = pd.concat([label_dict[str(the_label)], PM_data_year[station_id]], axis=1)\n",
    "    return label_dict\n",
    "\n",
    "def merge_to_one_year(label_dict):\n",
    "    new_label_dict =label_dict.copy()\n",
    "    all_val = 0\n",
    "    if 'Date' in new_label_dict.keys():\n",
    "        del new_label_dict['Date']\n",
    "    for key, df in new_label_dict.items():\n",
    "        if len(df.shape) > 1:\n",
    "            mean_data = df.mean(axis=1, numeric_only=True).to_numpy()\n",
    "        else:\n",
    "            mean_data = df.to_numpy()\n",
    "        if mean_data.shape[0] == 365:\n",
    "            mean_data = np.delete(mean_data, 58)\n",
    "        all_val += mean_data\n",
    "    return all_val / len(new_label_dict.keys())\n",
    "\n",
    "def merge_all_year_as_timeseries(PM_data):\n",
    "    all_year = pd.DataFrame(columns=['Date', 'Mean PM2.5'])\n",
    "    for key, df in PM_data.items():\n",
    "        mean_data = df.mean(axis=1, numeric_only=True)\n",
    "        a_year_df = pd.DataFrame(df['Date'].copy().to_list(), columns=['Date'])\n",
    "        a_year_df['Mean PM2.5'] = mean_data.copy().to_list()\n",
    "        all_year = pd.concat([all_year, a_year_df], axis=0)\n",
    "\n",
    "    # Add some detail\n",
    "    all_year = all_year.reset_index().drop(columns=['index'])\n",
    "    all_year['Date'] = all_year['Date'].apply(lambda x : datetime.strptime(x, '%Y_%m_%d'))\n",
    "    all_year = all_year.drop_duplicates()\n",
    "    all_year = all_year.set_index('Date').asfreq(freq='D')\n",
    "    all_year['Mean PM2.5'] = all_year['Mean PM2.5'].interpolate('linear')\n",
    "    return all_year"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteorology_labeling(provide_label_path):\n",
    "    NE = ' อำนาจเจริญ, บึงกาฬ, บุรีรัมย์, ชัยภูมิ, กาฬสินธุ์, ขอนแก่น, เลย, มหาสารคาม, มุกดาหาร, นครพนม, นครราชสีมา, หนองบัวลำภู, หนองคาย, ร้อยเอ็ด, สกลนคร, ศรีสะเกษ, สุรินทร์, อุบลราชธานี, อุดรธานี, ยโสธร'\n",
    "    N = ' เชียงใหม่, เชียงราย, ลำปาง, ลำพูน, แม่ฮ่องสอน, น่าน, พะเยา, แพร่, อุตรดิตถ์, ตาก, สุโขทัย, พิษณุโลก, พิจิตร, กำแพงเพชร, เพชรบูรณ์'\n",
    "    C = ' นครสวรรค์, อุทัยธานี, อ่างทอง, ชัยนาท, พระนครศรีอยุธยา, กทม., ลพบุรี, นครปฐม, นนทบุรี, ปทุมธานี, สมุทรปราการ, สมุทรสาคร, สมุทรสงคราม, สระบุรี, สิงห์บุรี, สุพรรณบุรี, กาญจนบุรี, ราชบุรี'\n",
    "    E = ' ฉะเชิงเทรา, จันทบุรี, ชลบุรี, ปราจีนบุรี, ระยอง, สระแก้ว, ตราด, นครนายก'\n",
    "    SE = ' เพชรบุรี, ประจวบคีรีขันธ์, ประจวบคิรีขันธ์, ชุมพร, นครศรีธรรมราช, นราธิวาส, ปัตตานี, พัทลุง, สงขลา, สุราษฏร์ธานี, ยะลา'\n",
    "    SW = ' กระบี่, พังงา, ภูเก็ต, ระนอง, สตูล, ตรัง'\n",
    "    Thai_sector = [NE, N, C, E, SE, SW]\n",
    "    Thai_dict = {f'{j}': ['จ.' + i.split(' ')[-1] if i.split(' ')[-1] != 'กทม.' else i.split(' ')[-1] for i in Thai_sector[j].split(',')] for j in range(len(Thai_sector))}\n",
    "    swap_Thai_dict = {provide:sector for sector in Thai_dict.keys() for provide in Thai_dict[sector]}\n",
    "    provide_df = provide_labeling(provide_label_path)\n",
    "    label_df = provide_df.copy()\n",
    "    label_df['label'] = label_df['station_name'].apply(lambda x: swap_Thai_dict[x])\n",
    "    return label_df\n",
    "\n",
    "def provide_labeling(provide_label_path):\n",
    "    try:\n",
    "        return pd.read_csv(provide_label_path)\n",
    "    except Exception:\n",
    "        print('Not Found :', provide_label_path, 'in This Directory')\n",
    "        return -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PM_mean_curve(label_dict):\n",
    "    new_label_dict =label_dict.copy()\n",
    "    if 'Date' in new_label_dict.keys():\n",
    "        del new_label_dict['Date']\n",
    "    for key, df in new_label_dict.items():\n",
    "        if len(df.shape) > 1:\n",
    "            mean_data = df.mean(axis=1, numeric_only=True)\n",
    "        else:\n",
    "            mean_data = df\n",
    "        plt.plot(mean_data, label=str(key))\n",
    "    plt.hlines(50, 0, 356, linestyles='dashed', label='Thailand NAAQ (50)', color='gray')\n",
    "    plt.hlines(15, 0, 356, linestyles='dashed', label='WHO guldline (15)', color='black')\n",
    "    plt.ylabel('PM2.5 micrograms/m**3')\n",
    "    plt.xlabel('Day')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_TS_all_year(TS_PM_data, model='additive', plot_result=False):\n",
    "    result = seasonal_decompose(TS_PM_data, model='multiplicative')\n",
    "    if plot_result:\n",
    "        result.plot()\n",
    "    trend_data = result.trend.copy().interpolate('linear')\n",
    "    trend_data = trend_data.fillna(trend_data.to_numpy().mean())\n",
    "    time_index = trend_data.index.to_numpy()\n",
    "    trend_df = pd.DataFrame({'Date':time_index, 'PM2.5':trend_data.to_numpy()})\n",
    "    trend_df['Group'] = trend_df['Date'].apply(lambda x: datetime.strftime(x, '%m_%d'))\n",
    "    trend_df.groupby(['Group']).mean().plot()\n",
    "    plt.hlines(50, 0, 356, linestyles='dashed', label='Thailand NAAQ (50)', color='black')\n",
    "    plt.hlines(15, 0, 356, linestyles='dashed', label='WHO guldline (15)', color='red')\n",
    "    plt.ylabel('PM2.5 micrograms/m**3')\n",
    "    plt.xlabel('Day')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_TS_3_seasons(TS_PM_data, model='additive', plot_result=False):\n",
    "    result = seasonal_decompose(TS_PM_data, model='multiplicative')\n",
    "    if plot_result:\n",
    "        result.plot()\n",
    "    trend_data = result.trend.copy().interpolate('linear')\n",
    "    trend_data = trend_data.fillna(trend_data.to_numpy().mean())\n",
    "    time_index = trend_data.index.to_numpy()\n",
    "    trend_df = pd.DataFrame({'Date':time_index, 'PM2.5':trend_data.to_numpy()})\n",
    "    trend_df['Group'] = trend_df['Date'].apply(lambda x: datetime.strftime(x, '%m_%d'))\n",
    "    overall_trends = trend_df.groupby(['Group']).mean()\n",
    "\n",
    "    # Addition for 3 seasons plotting\n",
    "    hot_season = overall_trends.loc['02_01':'05_01'].copy()\n",
    "    rain_season = overall_trends.loc['05_01':'10_01'].copy()\n",
    "    cold_season = pd.concat([overall_trends.loc['10_01':].copy(), overall_trends.loc[:'02_02'].copy()])\n",
    "    hot_season['IDX'] = np.arange(hot_season.index.shape[0])\n",
    "    rain_season['IDX'] = np.arange(rain_season.index.shape[0])\n",
    "    cold_season['IDX'] = np.arange(cold_season.index.shape[0])\n",
    "    hot_center = ['02_15', '03_15', '04_15']\n",
    "    rain_center = ['05_15', '06_15', '07_15', '08_15', '09_15']\n",
    "    cold_center = ['10_15', '11_15', '12_15', '01_15']\n",
    "    hot_idx = list(map(lambda x: int(hot_season.loc[x]['IDX']), hot_center))\n",
    "    rain_idx = list(map(lambda x: int(rain_season.loc[x]['IDX']), rain_center))\n",
    "    cold_idx = list(map(lambda x: int(cold_season.loc[x]['IDX']), cold_center))\n",
    "    hot_season = hot_season.drop(columns=['IDX'])\n",
    "    rain_season = rain_season.drop(columns=['IDX'])\n",
    "    cold_season = cold_season.drop(columns=['IDX'])\n",
    "\n",
    "    hot_season.plot()\n",
    "    plt.xticks(hot_idx, labels=hot_center)\n",
    "    plt.grid()\n",
    "    plt.title('Summer')\n",
    "    plt.ylabel('PM2.5 micrograms/m**3')\n",
    "    plt.xlabel('Day')\n",
    "    plt.legend()\n",
    "\n",
    "    rain_season.plot()\n",
    "    plt.xticks(rain_idx, labels=rain_center)\n",
    "    plt.grid()\n",
    "    plt.title('Rainy')\n",
    "    plt.ylabel('PM2.5 micrograms/m**3')\n",
    "    plt.xlabel('Day')\n",
    "    plt.legend()\n",
    "\n",
    "    cold_season.plot()\n",
    "    plt.xticks(cold_idx, labels=cold_center)\n",
    "    plt.grid()\n",
    "    plt.title('Winter')\n",
    "    plt.ylabel('PM2.5 micrograms/m**3')\n",
    "    plt.xlabel('Day')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_year_pollution_ratio(PM_data_year, cond_lst):\n",
    "    PM_data_year['Month'] = PM_data_year['Date'].apply(lambda x: x.split('_')[1])\n",
    "    month_series = PM_data_year['Month'].copy()\n",
    "    value_df = PM_data_year.copy().drop(columns=['Month', 'Date'])\n",
    "    count_per_month_df = pd.DataFrame(month_series.unique(), columns=['Month'])\n",
    "    for idx in range(1, len(cond_lst)):\n",
    "        upper = cond_lst[idx]\n",
    "        lower = cond_lst[idx - 1]\n",
    "        conditional_df = value_df.applymap(lambda x: 1 if x >= lower and x < upper else 0)\n",
    "        val = pd.concat([month_series, pd.Series(conditional_df.sum(axis=1), name=f'{lower}<=x<{upper}')], axis=1).groupby(['Month']).sum()\n",
    "        count_per_month_df = pd.concat([count_per_month_df, val[f'{lower}<=x<{upper}'].reset_index()], axis=1)\n",
    "        count_per_month_df = count_per_month_df.T.drop_duplicates().T\n",
    "        \n",
    "    # Normalize\n",
    "    count_per_month_df = count_per_month_df.astype(np.int32).drop(columns=['Month'])\n",
    "    sum_count_month = count_per_month_df.copy().sum(axis=1)\n",
    "    normalized_df = count_per_month_df.div(sum_count_month, axis=0)\n",
    "    month_df = pd.DataFrame(month_series.unique(), columns=['Month'])\n",
    "    return pd.concat([month_df, normalized_df.reindex(month_df.index)], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
